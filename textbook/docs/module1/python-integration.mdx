---
sidebar_position: 4
---

import {useTranslation} from 'react-i18next';

export default function PythonIntegration() {
  const {i18n} = useTranslation();
  const isUrdu = i18n.language === 'ur';
  
  if (isUrdu) {
    return (
      <div>
        <h1>Python Ø§ÛŒØ¬Ù†Ù¹Ø³ Ú©Ùˆ ROS 2 Ú©Û’ Ø³Ø§ØªÚ¾ Ø¬ÙˆÚ‘Ù†Ø§</h1>
        
        <h2>ğŸ¯ ØªØ¹Ø§Ø±Ù</h2>
        
        <p>ROS 2 Ú©Û’ Ø³Ø¨ Ø³Û’ Ø·Ø§Ù‚ØªÙˆØ± Ù¾ÛÙ„ÙˆØ¤Úº Ù…ÛŒÚº Ø³Û’ Ø§ÛŒÚ© <strong>Python Ù¾Ø± Ù…Ø¨Ù†ÛŒ AI Ø§ÛŒØ¬Ù†Ù¹Ø³</strong> Ú©Ùˆ Ø±ÙˆØ¨ÙˆÙ¹Ú© Ø³Ø³Ù¹Ù…Ø² Ú©Û’ Ø³Ø§ØªÚ¾ Ù…Ø±Ø¨ÙˆØ· Ú©Ø±Ù†Û’ Ú©ÛŒ ØµÙ„Ø§Ø­ÛŒØª ÛÛ’Û” ÛŒÛ Ø¨Ø§Ø¨ Ø¢Ù¾ Ú©Ùˆ Ø¯Ú©Ú¾Ø§ØªØ§ ÛÛ’ Ú©Û Ø°ÛÛŒÙ† Ø±ÙˆØ¨ÙˆÙ¹ Ú©Ù†Ù¹Ø±ÙˆÙ„Ø±Ø² Ø¨Ù†Ø§Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ PythonØŒ <code>rclpy</code>ØŒ Ø§ÙˆØ± Ø¬Ø¯ÛŒØ¯ AI Ù„Ø§Ø¦Ø¨Ø±ÛŒØ±ÛŒÙˆÚº Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©ÛŒØ³Û’ Ú©Ø±ÛŒÚºÛ”</p>
        
        <h2>ğŸ Ø±ÙˆØ¨ÙˆÙ¹Ú©Ø³ Ú©Û’ Ù„ÛŒÛ’ Python Ú©ÛŒÙˆÚºØŸ</h2>
        
        <p>Python Ø±ÙˆØ¨ÙˆÙ¹Ú©Ø³ AI Ú©Û’ Ù„ÛŒÛ’ Ù…Ø«Ø§Ù„ÛŒ ÛÛ’ Ú©ÛŒÙˆÙ†Ú©Û:</p>
        
        <ul>
          <li>âœ… <strong>Ø§Ù…ÛŒØ± AI Ø§ÛŒÚ©Ùˆ Ø³Ø³Ù¹Ù…</strong>: TensorFlow, PyTorch, scikit-learn</li>
          <li>âœ… <strong>ØªÛŒØ² Ù¾Ø±ÙˆÙ¹Ùˆ Ù¹Ø§Ø¦Ù¾Ù†Ú¯</strong>: Ø§Ù„Ú¯ÙˆØ±ØªÚ¾Ù… Ù¾Ø± ÙÙˆØ±ÛŒ ØªÚ©Ø±Ø§Ø±</li>
          <li>âœ… <strong>Ø¢Ø³Ø§Ù† Ø§Ù†Ø¶Ù…Ø§Ù…</strong>: rclpy Ú©Û’ Ø°Ø±ÛŒØ¹Û’ ROS 2 Ú©Û’ Ø³Ø§ØªÚ¾ ÛÙ…ÙˆØ§Ø±</li>
          <li>âœ… <strong>Ú©Ù…ÛŒÙˆÙ†Ù¹ÛŒ</strong>: ÙˆØ³ÛŒØ¹ Ù„Ø§Ø¦Ø¨Ø±ÛŒØ±ÛŒØ§Úº Ø§ÙˆØ± ÙˆØ³Ø§Ø¦Ù„</li>
          <li>âš ï¸ <strong>Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ Ú©Ø§ Ù¹Ø±ÛŒÚˆ Ø¢Ù</strong>: Ø­Ù‚ÛŒÙ‚ÛŒ ÙˆÙ‚Øª Ú©Û’ Ø§ÛÙ… Ø±Ø§Ø³ØªÙˆÚº Ú©Û’ Ù„ÛŒÛ’ C++ Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº</li>
        </ul>
        
        <h2>ğŸ”Œ rclpy Ù„Ø§Ø¦Ø¨Ø±ÛŒØ±ÛŒ</h2>
        
        <p><code>rclpy</code> ROS 2 Ú©Û’ Ù„ÛŒÛ’ Python Ú©Ù„Ø§Ø¦Ù†Ù¹ Ù„Ø§Ø¦Ø¨Ø±ÛŒØ±ÛŒ ÛÛ’Û” ÛŒÛ ÙØ±Ø§ÛÙ… Ú©Ø±ØªØ§ ÛÛ’:</p>
        
        <ul>
          <li>Ù†ÙˆÚˆ Ú©ÛŒ ØªØ®Ù„ÛŒÙ‚ Ø§ÙˆØ± Ø§Ù†ØªØ¸Ø§Ù…</li>
          <li>Ù¾Ø¨Ù„Ø´Ø±Ø²ØŒ Ø³Ø¨Ø³Ú©Ø±Ø§Ø¦Ø¨Ø±Ø²ØŒ Ø³Ø±ÙˆØ³Ø²ØŒ Ø§ÛŒÚ©Ø´Ù†Ø²</li>
          <li>Ù¾ÛŒØ±Ø§Ù…ÛŒÙ¹Ø±Ø² Ø§ÙˆØ± Ù„Ø§Ú¯Ù†Ú¯</li>
          <li>Ù¹Ø§Ø¦Ù…Ø± Ø§ÙˆØ± Ø§ÛŒÚ¯Ø²ÛŒÚ©ÛŒÙˆÙ¹Ø±Ø²</li>
        </ul>
        
        <h3>ØªÙ†ØµÛŒØ¨</h3>
        
        <pre>{`
# Usually installed with ROS 2
sudo apt install python3-rclpy

# Additional useful packages
sudo apt install python3-sensor-msgs python3-geometry-msgs
        `}</pre>
        
        <h2>ğŸ¤– Ø§ÛŒÚ© AI Ø³Û’ Ú†Ù„Ù†Û’ ÙˆØ§Ù„Ø§ Ù†ÙˆÚˆ Ø¨Ù†Ø§Ù†Ø§</h2>
        
        <h3>Ù…Ø«Ø§Ù„: Ø¢Ø¨Ø¬ÛŒÚ©Ù¹ ÚˆÛŒÙ¹ÛŒÚ©Ø´Ù† Ù†ÙˆÚˆ</h3>
        
        <p>ÛŒÛ Ù…Ø«Ø§Ù„ Ø§ÛŒÚ© Ø³Ø§Ø¯Û AI Ù…Ø§ÚˆÙ„ Ú©Ùˆ ROS 2 Ú©Û’ Ø³Ø§ØªÚ¾ Ù…Ø±Ø¨ÙˆØ· Ú©Ø±ØªÛŒ ÛÛ’:</p>
        
        <pre>{`
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image
from vision_msgs.msg import Detection2DArray, Detection2D
from cv_bridge import CvBridge
import cv2
import numpy as np

class ObjectDetectorNode(Node):
    def __init__(self):
        super().__init__('object_detector')

        # Declare parameters
        self.declare_parameter('confidence_threshold', 0.5)
        self.declare_parameter('model_path', '/path/to/model.pt')

        # Create CV Bridge
        self.bridge = CvBridge()

        # Subscribe to camera feed
        self.image_sub = self.create_subscription(
            Image,
            '/camera/image_raw',
            self.image_callback,
            10
        )

        # Publish detections
        self.detection_pub = self.create_publisher(
            Detection2DArray,
            '/detected_objects',
            10
        )

        # Load AI model (placeholder - use your actual model)
        self.model = self.load_model()

        self.get_logger().info('Object Detector Node Initialized')

    def load_model(self):
        """Load your AI model here (PyTorch, TensorFlow, etc.)"""
        model_path = self.get_parameter('model_path').value
        # model = torch.load(model_path)
        # return model
        return None  # Placeholder

    def image_callback(self, msg):
        """Process incoming images"""
        try:
            # Convert ROS Image to OpenCV
            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')

            # Run detection
            detections = self.detect_objects(cv_image)

            # Publish detections
            self.publish_detections(detections)

        except Exception as e:
            self.get_logger().error(f'Error processing image: {str(e)}')

    def detect_objects(self, image):
        """Run AI model inference"""
        # Preprocess image
        # input_tensor = self.preprocess(image)

        # Run model
        # with torch.no_grad():
        #     predictions = self.model(input_tensor)

        # Post-process results
        # detections = self.postprocess(predictions)

        # Placeholder return
        return []

    def publish_detections(self, detections):
        """Publish detection results"""
        msg = Detection2DArray()
        msg.header.stamp = self.get_clock().now().to_msg()
        msg.header.frame_id = 'camera_frame'

        for det in detections:
            detection = Detection2D()
            # Fill in detection data
            msg.detections.append(detection)

        self.detection_pub.publish(msg)

def main(args=None):
    rclpy.init(args=args)
    node = ObjectDetectorNode()
    rclpy.spin(node)
    node.destroy_node()
    rclpy.shutdown()

if __name__ == '__main__':
    main()
        `}</pre>
        
        <h2>ğŸ§  LLMs Ú©Ùˆ ROS 2 Ú©Û’ Ø³Ø§ØªÚ¾ Ø¶Ù… Ú©Ø±Ù†Ø§</h2>
        
        <h3>Ù…Ø«Ø§Ù„: ÙˆØ§Ø¦Ø³ Ú©Ù…Ø§Ù†Úˆ Ù¾Ø±ÙˆØ³ÛŒØ³Ø±</h3>
        
        <pre>{`
import rclpy
from rclpy.node import Node
from std_msgs.msg import String
from geometry_msgs.msg import Twist
import openai

class VoiceCommandNode(Node):
    def __init__(self):
        super().__init__('voice_command_processor')

        # Subscribe to speech recognition output
        self.voice_sub = self.create_subscription(
            String,
            '/voice/recognized_text',
            self.voice_callback,
            10
        )

        # Publish movement commands
        self.cmd_pub = self.create_publisher(
            Twist,
            '/cmd_vel',
            10
        )

        self.get_logger().info('Voice Command Processor Ready')

    def voice_callback(self, msg):
        """Process voice commands using LLM"""
        command_text = msg.data
        self.get_logger().info(f'Received command: {command_text}')

        # Use LLM to understand intent
        robot_action = self.parse_command_with_llm(command_text)

        # Execute action
        self.execute_action(robot_action)

    def parse_command_with_llm(self, text):
        """Use LLM to parse natural language commands"""
        prompt = f"""
        You are a robot controller. Parse this command into a structured action:
        Command: "{text}"

        Return JSON with: {{action: "move/turn/stop", direction: "forward/backward/left/right", speed: 0-1}}
        """

        # Call OpenAI API (placeholder)
        # response = openai.Completion.create(...)
        # return parse_json(response)

        return {"action": "move", "direction": "forward", "speed": 0.5}

    def execute_action(self, action):
        """Convert action to ROS command"""
        cmd = Twist()

        if action['action'] == 'move':
            if action['direction'] == 'forward':
                cmd.linear.x = action['speed']
            elif action['direction'] == 'backward':
                cmd.linear.x = -action['speed']
        elif action['action'] == 'turn':
            if action['direction'] == 'left':
                cmd.angular.z = action['speed']
            elif action['direction'] == 'right':
                cmd.angular.z = -action['speed']

        self.cmd_pub.publish(cmd)
        self.get_logger().info(f'Executing: {action}')
        `}</pre>
        
        <h2>ğŸ› ï¸ Ù¾ÛŒØ±Ø§Ù…ÛŒÙ¹Ø±Ø² Ú©Û’ Ø³Ø§ØªÚ¾ Ú©Ø§Ù… Ú©Ø±Ù†Ø§</h2>
        
        <p>Ù¾ÛŒØ±Ø§Ù…ÛŒÙ¹Ø±Ø² Ø±Ù† Ù¹Ø§Ø¦Ù… Ú©Ù†ÙÛŒÚ¯Ø±ÛŒØ´Ù† Ú©ÛŒ Ø§Ø¬Ø§Ø²Øª Ø¯ÛŒØªÛ’ ÛÛŒÚº:</p>
        
        <pre>{`
class ConfigurableNode(Node):
    def __init__(self):
        super().__init__('configurable_node')

        # Declare parameters with defaults
        self.declare_parameter('update_rate', 10.0)
        self.declare_parameter('robot_name', 'my_robot')
        self.declare_parameter('enable_ai', True)

        # Get parameter values
        rate = self.get_parameter('update_rate').value
        name = self.get_parameter('robot_name').value
        ai_enabled = self.get_parameter('enable_ai').value

        self.get_logger().info(f'Robot: {name}, Rate: {rate}Hz, AI: {ai_enabled}')

        # Add parameter callback for dynamic updates
        self.add_on_set_parameters_callback(self.parameter_callback)

    def parameter_callback(self, params):
        """Called when parameters change"""
        for param in params:
            self.get_logger().info(f'Parameter changed: {param.name} = {param.value}')
        return True  # Accept changes
        `}</pre>
        
        <p>Ø±Ù† Ù¹Ø§Ø¦Ù… Ù¾Ø± Ù¾ÛŒØ±Ø§Ù…ÛŒÙ¹Ø±Ø² Ø³ÛŒÙ¹ Ú©Ø±ÛŒÚº:</p>
        
        <pre>{`
# Set parameter via command line
ros2 param set /configurable_node update_rate 20.0

# Load from YAML file
ros2 run my_package my_node --ros-args --params-file config.yaml
        `}</pre>
        
        <h2>ğŸš€ Ø¨ÛØªØ±ÛŒÙ† Ø·Ø±ÛŒÙ‚Û’</h2>
        
        <h3>1. Ù…Ù„Ù¹ÛŒ ØªÚ¾Ø±ÛŒÚˆÙ†Ú¯ Ú©Û’ Ù„ÛŒÛ’ Ø§ÛŒÚ¯Ø²ÛŒÚ©ÛŒÙˆÙ¹Ø±Ø² Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº</h3>
        
        <pre>{`
from rclpy.executors import MultiThreadedExecutor

def main():
    rclpy.init()

    node1 = SensorNode()
    node2 = ControlNode()

    executor = MultiThreadedExecutor()
    executor.add_node(node1)
    executor.add_node(node2)

    try:
        executor.spin()
    finally:
        executor.shutdown()
        node1.destroy_node()
        node2.destroy_node()
        rclpy.shutdown()
        `}</pre>
        
        <h3>2. Ù„Ø§Ø¦Ù Ø³Ø§Ø¦ÛŒÚ©Ù„ Ú©Ùˆ ØµØ­ÛŒØ­ Ø·Ø±ÛŒÙ‚Û’ Ø³Û’ ÛÛŒÙ†ÚˆÙ„ Ú©Ø±ÛŒÚº</h3>
        
        <pre>{`
import signal

class RobustNode(Node):
    def __init__(self):
        super().__init__('robust_node')
        self.running = True

        # Set up signal handler
        signal.signal(signal.SIGINT, self.signal_handler)

    def signal_handler(self, sig, frame):
        self.get_logger().info('Shutting down gracefully...')
        self.running = False

    def cleanup(self):
        """Cleanup resources"""
        # Close connections, save state, etc.
        pass
        `}</pre>
        
        <h3>3. Ù„Ø§Ú¯Ù†Ú¯ Ú©Û’ Ø¨ÛØªØ±ÛŒÙ† Ø·Ø±ÛŒÙ‚Û’</h3>
        
        <pre>{`
# Different log levels
self.get_logger().debug('Detailed debug information')
self.get_logger().info('General information')
self.get_logger().warn('Warning message')
self.get_logger().error('Error occurred')
self.get_logger().fatal('Critical error')
        `}</pre>
        
        <h2>ğŸ”— Ø§Ù†Ø¶Ù…Ø§Ù… Ú©Û’ Ù†Ù…ÙˆÙ†Û’</h2>
        
        <h3>Ù¾ÛŒÙ¹Ø±Ù† 1: Ø§Ø¯Ø±Ø§Ú©-Ù…Ù†ØµÙˆØ¨Û Ø¨Ù†Ø¯ÛŒ-Ú©Ù†Ù¹Ø±ÙˆÙ„</h3>
        
        <pre>
[Camera] â†’ [AI Detector] â†’ [Path Planner] â†’ [Motor Controller]
        </pre>
        
        <h3>Ù¾ÛŒÙ¹Ø±Ù† 2: Ø³ÛŒÙ†Ø³Ø± ÙÛŒÙˆÚ˜Ù†</h3>
        
        <pre>
[Lidar] â†˜
[Camera] â†’ [Fusion Node] â†’ [SLAM] â†’ [Navigation]
[IMU]   â†—
        </pre>
        
        <h3>Ù¾ÛŒÙ¹Ø±Ù† 3: Ø³Ù„ÙˆÚ© Ú©Ø§ Ø¯Ø±Ø®Øª (Behavior Tree)</h3>
        
        <pre>{`
class BehaviorTreeNode(Node):
    def __init__(self):
        super().__init__('behavior_tree')
        self.state = 'IDLE'

        self.timer = self.create_timer(0.1, self.execute_behavior)

    def execute_behavior(self):
        if self.state == 'IDLE':
            self.check_for_task()
        elif self.state == 'NAVIGATING':
            self.navigate_to_goal()
        elif self.state == 'MANIPULATING':
            self.perform_manipulation()
        `}</pre>
        
        <h2>ğŸ“š Ø§Ú¯Ù„Û’ Ø§Ù‚Ø¯Ø§Ù…Ø§Øª</h2>
        
        <p>Ø§Ø¨ Ø¬Ø¨ Ú©Û Ø¢Ù¾ Python AI Ú©Ùˆ ROS 2 Ú©Û’ Ø³Ø§ØªÚ¾ Ø¶Ù… Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºØŒ Ø±ÙˆØ¨ÙˆÙ¹ Ú©Û’ ÚˆÚ¾Ø§Ù†Ú†Û’ Ú©Ùˆ Ø¨ÛŒØ§Ù† Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ <strong>URDF</strong> Ú©Û’ Ø¨Ø§Ø±Û’ Ù…ÛŒÚº Ø¬Ø§Ù†ÛŒÚº â†’</p>
      </div>
    );
  }
  
  return (
    <div>
      <h1>Bridging Python Agents with ROS 2</h1>
      
      <h2>ğŸ¯ Introduction</h2>
      
      <p>One of the most powerful aspects of ROS 2 is the ability to integrate <strong>Python-based AI agents</strong> with robotic systems. This chapter shows you how to use Python, <code>rclpy</code>, and modern AI libraries to create intelligent robot controllers.</p>
      
      <h2>ğŸ Why Python for Robotics?</h2>
      
      <p>Python is ideal for robotics AI because:</p>
      
      <ul>
        <li>âœ… <strong>Rich AI Ecosystem</strong>: TensorFlow, PyTorch, scikit-learn</li>
        <li>âœ… <strong>Rapid Prototyping</strong>: Quick iteration on algorithms</li>
        <li>âœ… <strong>Easy Integration</strong>: Seamless with ROS 2 via rclpy</li>
        <li>âœ… <strong>Community</strong>: Vast libraries and resources</li>
        <li>âš ï¸ <strong>Performance Trade-off</strong>: Use C++ for real-time critical paths</li>
      </ul>
      
      <h2>ğŸ”Œ The rclpy Library</h2>
      
      <p><code>rclpy</code> is the Python client library for ROS 2. It provides:</p>
      
      <ul>
        <li>Node creation and management</li>
        <li>Publishers, Subscribers, Services, Actions</li>
        <li>Parameters and logging</li>
        <li>Timers and executors</li>
      </ul>
      
      <h3>Installation</h3>
      
      <pre>{`
# Usually installed with ROS 2
sudo apt install python3-rclpy

# Additional useful packages
sudo apt install python3-sensor-msgs python3-geometry-msgs
      `}</pre>
      
      <h2>ğŸ¤– Creating an AI-Powered Node</h2>
      
      <h3>Example: Object Detection Node</h3>
      
      <p>This example integrates a simple AI model with ROS 2:</p>
      
      <pre>{`
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image
from vision_msgs.msg import Detection2DArray, Detection2D
from cv_bridge import CvBridge
import cv2
import numpy as np

class ObjectDetectorNode(Node):
    def __init__(self):
        super().__init__('object_detector')

        # Declare parameters
        self.declare_parameter('confidence_threshold', 0.5)
        self.declare_parameter('model_path', '/path/to/model.pt')

        # Create CV Bridge
        self.bridge = CvBridge()

        # Subscribe to camera feed
        self.image_sub = self.create_subscription(
            Image,
            '/camera/image_raw',
            self.image_callback,
            10
        )

        # Publish detections
        self.detection_pub = self.create_publisher(
            Detection2DArray,
            '/detected_objects',
            10
        )

        # Load AI model (placeholder - use your actual model)
        self.model = self.load_model()

        self.get_logger().info('Object Detector Node Initialized')

    def load_model(self):
        """Load your AI model here (PyTorch, TensorFlow, etc.)"""
        model_path = self.get_parameter('model_path').value
        # model = torch.load(model_path)
        # return model
        return None  # Placeholder

    def image_callback(self, msg):
        """Process incoming images"""
        try:
            # Convert ROS Image to OpenCV
            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')

            # Run detection
            detections = self.detect_objects(cv_image)

            # Publish detections
            self.publish_detections(detections)

        except Exception as e:
            self.get_logger().error(f'Error processing image: {str(e)}')

    def detect_objects(self, image):
        """Run AI model inference"""
        # Preprocess image
        # input_tensor = self.preprocess(image)

        # Run model
        # with torch.no_grad():
        #     predictions = self.model(input_tensor)

        # Post-process results
        # detections = self.postprocess(predictions)

        # Placeholder return
        return []

    def publish_detections(self, detections):
        """Publish detection results"""
        msg = Detection2DArray()
        msg.header.stamp = self.get_clock().now().to_msg()
        msg.header.frame_id = 'camera_frame'

        for det in detections:
            detection = Detection2D()
            # Fill in detection data
            msg.detections.append(detection)

        self.detection_pub.publish(msg)

def main(args=None):
    rclpy.init(args=args)
    node = ObjectDetectorNode()
    rclpy.spin(node)
    node.destroy_node()
    rclpy.shutdown()

if __name__ == '__main__':
    main()
      `}</pre>
      
      <h2>ğŸ§  Integrating LLMs with ROS 2</h2>
      
      <h3>Example: Voice Command Processor</h3>
      
      <pre>{`
import rclpy
from rclpy.node import Node
from std_msgs.msg import String
from geometry_msgs.msg import Twist
import openai

class VoiceCommandNode(Node):
    def __init__(self):
        super().__init__('voice_command_processor')

        # Subscribe to speech recognition output
        self.voice_sub = self.create_subscription(
            String,
            '/voice/recognized_text',
            self.voice_callback,
            10
        )

        # Publish movement commands
        self.cmd_pub = self.create_publisher(
            Twist,
            '/cmd_vel',
            10
        )

        self.get_logger().info('Voice Command Processor Ready')

    def voice_callback(self, msg):
        """Process voice commands using LLM"""
        command_text = msg.data
        self.get_logger().info(f'Received command: {command_text}')

        # Use LLM to understand intent
        robot_action = self.parse_command_with_llm(command_text)

        # Execute action
        self.execute_action(robot_action)

    def parse_command_with_llm(self, text):
        """Use LLM to parse natural language commands"""
        prompt = f"""
        You are a robot controller. Parse this command into a structured action:
        Command: "{text}"

        Return JSON with: {{action: "move/turn/stop", direction: "forward/backward/left/right", speed: 0-1}}
        """

        # Call OpenAI API (placeholder)
        # response = openai.Completion.create(...)
        # return parse_json(response)

        return {"action": "move", "direction": "forward", "speed": 0.5}

    def execute_action(self, action):
        """Convert action to ROS command"""
        cmd = Twist()

        if action['action'] == 'move':
            if action['direction'] == 'forward':
                cmd.linear.x = action['speed']
            elif action['direction'] == 'backward':
                cmd.linear.x = -action['speed']
        elif action['action'] == 'turn':
            if action['direction'] == 'left':
                cmd.angular.z = action['speed']
            elif action['direction'] == 'right':
                cmd.angular.z = -action['speed']

        self.cmd_pub.publish(cmd)
        self.get_logger().info(f'Executing: {action}')
      `}</pre>
      
      <h2>ğŸ› ï¸ Working with Parameters</h2>
      
      <p>Parameters allow runtime configuration:</p>
      
      <pre>{`
class ConfigurableNode(Node):
    def __init__(self):
        super().__init__('configurable_node')

        # Declare parameters with defaults
        self.declare_parameter('update_rate', 10.0)
        self.declare_parameter('robot_name', 'my_robot')
        self.declare_parameter('enable_ai', True)

        # Get parameter values
        rate = self.get_parameter('update_rate').value
        name = self.get_parameter('robot_name').value
        ai_enabled = self.get_parameter('enable_ai').value

        self.get_logger().info(f'Robot: {name}, Rate: {rate}Hz, AI: {ai_enabled}')

        # Add parameter callback for dynamic updates
        self.add_on_set_parameters_callback(self.parameter_callback)

    def parameter_callback(self, params):
        """Called when parameters change"""
        for param in params:
            self.get_logger().info(f'Parameter changed: {param.name} = {param.value}')
        return True  # Accept changes
      `}</pre>
      
      <p>Set parameters at runtime:</p>
      
      <pre>{`
# Set parameter via command line
ros2 param set /configurable_node update_rate 20.0

# Load from YAML file
ros2 run my_package my_node --ros-args --params-file config.yaml
      `}</pre>
      
      <h2>ğŸš€ Best Practices</h2>
      
      <h3>1. Use Executors for Multi-threading</h3>
      
      <pre>{`
from rclpy.executors import MultiThreadedExecutor

def main():
    rclpy.init()

    node1 = SensorNode()
    node2 = ControlNode()

    executor = MultiThreadedExecutor()
    executor.add_node(node1)
    executor.add_node(node2)

    try:
        executor.spin()
    finally:
        executor.shutdown()
        node1.destroy_node()
        node2.destroy_node()
        rclpy.shutdown()
      `}</pre>
      
      <h3>2. Handle Lifecycle Properly</h3>
      
      <pre>{`
import signal

class RobustNode(Node):
    def __init__(self):
        super().__init__('robust_node')
        self.running = True

        # Set up signal handler
        signal.signal(signal.SIGINT, self.signal_handler)

    def signal_handler(self, sig, frame):
        self.get_logger().info('Shutting down gracefully...')
        self.running = False

    def cleanup(self):
        """Cleanup resources"""
        # Close connections, save state, etc.
        pass
      `}</pre>
      
      <h3>3. Logging Best Practices</h3>
      
      <pre>{`
# Different log levels
self.get_logger().debug('Detailed debug information')
self.get_logger().info('General information')
self.get_logger().warn('Warning message')
self.get_logger().error('Error occurred')
self.get_logger().fatal('Critical error')
      `}</pre>
      
      <h2>ğŸ”— Integration Patterns</h2>
      
      <h3>Pattern 1: Perception-Planning-Control</h3>
      
      <pre>
[Camera] â†’ [AI Detector] â†’ [Path Planner] â†’ [Motor Controller]
      </pre>
      
      <h3>Pattern 2: Sensor Fusion</h3>
      
      <pre>
[Lidar] â†˜
[Camera] â†’ [Fusion Node] â†’ [SLAM] â†’ [Navigation]
[IMU]   â†—
      </pre>
      
      <h3>Pattern 3: Behavior Tree</h3>
      
      <pre>{`
class BehaviorTreeNode(Node):
    def __init__(self):
        super().__init__('behavior_tree')
        self.state = 'IDLE'

        self.timer = self.create_timer(0.1, self.execute_behavior)

    def execute_behavior(self):
        if self.state == 'IDLE':
            self.check_for_task()
        elif self.state == 'NAVIGATING':
            self.navigate_to_goal()
        elif self.state == 'MANIPULATING':
            self.perform_manipulation()
      `}</pre>
      
      <h2>ğŸ“š Next Steps</h2>
      
      <p>Now that you can integrate Python AI with ROS 2, learn about <strong>URDF</strong> for describing robot structures â†’</p>
    </div>
  );
}
