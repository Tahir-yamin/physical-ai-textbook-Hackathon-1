---
sidebar_position: 2
---

import {useTranslation} from 'react-i18next';

export default function WhisperIntegration() {
  const {i18n} = useTranslation();
  const isUrdu = i18n.language === 'ur';
  
  if (isUrdu) {
    return (
      <div>
        <h1>Whisper Ú©Û’ Ø³Ø§ØªÚ¾ ØªÙ‚Ø±ÛŒØ± Ú©ÛŒ Ø´Ù†Ø§Ø®Øª (Speech Recognition)</h1>
        
        <h2>ğŸ¯ OpenAI Whisper Ú©ÛŒØ§ ÛÛ’ØŸ</h2>
        
        <p><strong>Whisper</strong> Ø§ÛŒÚ© Ø¬Ø¯ÛŒØ¯ ØªØ±ÛŒÙ† ØªÙ‚Ø±ÛŒØ± Ú©ÛŒ Ø´Ù†Ø§Ø®Øª Ú©Ø§ Ù…Ø§ÚˆÙ„ ÛÛ’ Ø¬Ùˆ OpenAI Ú©Û’ Ø°Ø±ÛŒØ¹Û ØªÛŒØ§Ø± Ú©ÛŒØ§ Ú¯ÛŒØ§ ÛÛ’Û” ÛŒÛ Ø§Ù†ØªÛØ§Ø¦ÛŒ Ø¯Ø±Ø³Øª ÛÛ’ØŒ Ù…ØªØ¹Ø¯Ø¯ Ø²Ø¨Ø§Ù†ÙˆÚº (Ø¨Ø´Ù…ÙˆÙ„ Ø§Ø±Ø¯Ùˆ!) Ú©Ùˆ Ø³Ù¾ÙˆØ±Ù¹ Ú©Ø±ØªØ§ ÛÛ’ØŒ Ø§ÙˆØ± Ø´ÙˆØ± ÙˆØ§Ù„Û’ Ù…Ø§Ø­ÙˆÙ„ Ù…ÛŒÚº Ù…Ø¶Ø¨ÙˆØ· ÛÛ’Û” Ø±ÙˆØ¨ÙˆÙ¹Ú©Ø³ Ú©Û’ Ù„ÛŒÛ’ØŒ ÛŒÛ ÛÙ…ÛŒÚº Ø±ÙˆØ¨ÙˆÙ¹Ø³ Ú©Ùˆ Ø²Ø¨Ø§Ù†ÛŒ Ø§Ø­Ú©Ø§Ù…Ø§Øª Ø¯ÛŒÙ†Û’ Ú©ÛŒ Ø§Ø¬Ø§Ø²Øª Ø¯ÛŒØªØ§ ÛÛ’Û”</p>
        
        <h2>ğŸ’» ØªÙ†ØµÛŒØ¨</h2>
        
        <p>ÛÙ… ØªÛŒØ² Ø±ÙØªØ§Ø± Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ Ú©Û’ Ù„ÛŒÛ’ <code>faster-whisper</code> Ù„Ø§Ø¦Ø¨Ø±ÛŒØ±ÛŒ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº Ú¯Û’:</p>
        
        <pre>{`
pip install faster-whisper sounddevice numpy
sudo apt install libportaudio2
        `}</pre>
        
        <h2>ğŸ¤ Ø¢ÚˆÛŒÙˆ Ø±ÛŒÚ©Ø§Ø±ÚˆÙ†Ú¯</h2>
        
        <p>Ø³Ø¨ Ø³Û’ Ù¾ÛÙ„Û’ØŒ ÛÙ…ÛŒÚº Ù…Ø§Ø¦ÛŒÚ©Ø±ÙˆÙÙˆÙ† Ø³Û’ Ø¢ÚˆÛŒÙˆ Ú©ÛŒÙ¾Ú†Ø± Ú©Ø±Ù†Û’ Ú©ÛŒ Ø¶Ø±ÙˆØ±Øª ÛÛ’:</p>
        
        <pre>{`
import sounddevice as sd
import numpy as np

def record_audio(duration=5, fs=16000):
    print("Recording...")
    recording = sd.rec(int(duration * fs), samplerate=fs, channels=1)
    sd.wait()
    print("Finished recording")
    return recording.flatten()
        `}</pre>
        
        <h2>ğŸ§  Ù¹Ø±Ø§Ù†Ø³Ú©Ø±Ù¾Ø´Ù† (Transcription)</h2>
        
        <p>Ø§Ø¨ØŒ Ø¢ÚˆÛŒÙˆ Ú©Ùˆ Ù…ØªÙ† Ù…ÛŒÚº ØªØ¨Ø¯ÛŒÙ„ Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Whisper Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚº:</p>
        
        <pre>{`
from faster_whisper import WhisperModel

# Load model (tiny, base, small, medium, large)
model = WhisperModel("small", device="cpu", compute_type="int8")

def transcribe(audio_data):
    segments, info = model.transcribe(audio_data, beam_size=5)
    
    full_text = ""
    for segment in segments:
        full_text += segment.text + " "
        
    return full_text.strip()
        `}</pre>
        
        <h2>ğŸ¤– ROS 2 Ù†ÙˆÚˆ</h2>
        
        <p>Ø¢Ø¦ÛŒÛ’ Ø§Ø³Û’ ROS 2 Ù†ÙˆÚˆ Ù…ÛŒÚº Ù„Ù¾ÛŒÙ¹ØªÛ’ ÛÛŒÚº Ø¬Ùˆ Ù…ØªÙ† Ú©Ùˆ Ø´Ø§Ø¦Ø¹ Ú©Ø±ØªØ§ ÛÛ’:</p>
        
        <pre>{`
import rclpy
from rclpy.node import Node
from std_msgs.msg import String

class SpeechToTextNode(Node):
    def __init__(self):
        super().__init__('speech_to_text')
        self.publisher = self.create_publisher(String, '/voice/text', 10)
        self.timer = self.create_timer(1.0, self.listen_and_publish)
        self.model = WhisperModel("small", device="cpu", compute_type="int8")

    def listen_and_publish(self):
        # Record and transcribe (simplified)
        audio = record_audio()
        text = transcribe(audio)
        
        if text:
            msg = String()
            msg.data = text
            self.publisher.publish(msg)
            self.get_logger().info(f'Published: "{text}"')

def main():
    rclpy.init()
    node = SpeechToTextNode()
    rclpy.spin(node)
    node.destroy_node()
    rclpy.shutdown()
        `}</pre>
        
        <h2>ğŸŒ Ú©Ø«ÛŒØ± Ù„Ø³Ø§Ù†ÛŒ Ø³Ù¾ÙˆØ±Ù¹</h2>
        
        <p>Whisper Ø®ÙˆØ¯ Ø¨Ø®ÙˆØ¯ Ø²Ø¨Ø§Ù† Ú©Ø§ Ù¾ØªÛ Ù„Ú¯Ø§ Ø³Ú©ØªØ§ ÛÛ’Û” Ø§Ø±Ø¯Ùˆ Ú©Û’ Ù„ÛŒÛ’ØŒ Ø¢Ù¾ Ø§Ø³Û’ Ù…Ø¬Ø¨ÙˆØ± Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚº:</p>
        
        <pre>{`
segments, info = model.transcribe(audio, language="ur")
        `}</pre>
        
        <h2>ğŸ“š Ø§Ú¯Ù„Û’ Ø§Ù‚Ø¯Ø§Ù…Ø§Øª</h2>
        
        <p>Ø§Ø¨ Ø¬Ø¨ Ú©Û Ø±ÙˆØ¨ÙˆÙ¹ Ø³Ù† Ø³Ú©ØªØ§ ÛÛ’ØŒ Ø¢Ø¦ÛŒÛ’ Ø¯ÛŒÚ©Ú¾ØªÛ’ ÛÛŒÚº Ú©Û ÙˆÛ <strong>LLMs</strong> Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’ Ú©ÛŒØ³Û’ Ø³Ù…Ø¬Ú¾ Ø³Ú©ØªØ§ ÛÛ’ â†’</p>
      </div>
    );
  }
  
  return (
    <div>
      <h1>Speech Recognition with Whisper</h1>
      
      <h2>ğŸ¯ What is OpenAI Whisper?</h2>
      
      <p><strong>Whisper</strong> is a state-of-the-art speech recognition model developed by OpenAI. It is highly accurate, supports multiple languages (including Urdu!), and is robust to noise. For robotics, it allows us to give verbal commands to robots.</p>
      
      <h2>ğŸ’» Installation</h2>
      
      <p>We will use the <code>faster-whisper</code> library for better performance:</p>
      
      <pre>{`
pip install faster-whisper sounddevice numpy
sudo apt install libportaudio2
      `}</pre>
      
      <h2>ğŸ¤ Recording Audio</h2>
      
      <p>First, we need to capture audio from the microphone:</p>
      
      <pre>{`
import sounddevice as sd
import numpy as np

def record_audio(duration=5, fs=16000):
    print("Recording...")
    recording = sd.rec(int(duration * fs), samplerate=fs, channels=1)
    sd.wait()
    print("Finished recording")
    return recording.flatten()
      `}</pre>
      
      <h2>ğŸ§  Transcription</h2>
      
      <p>Now, use Whisper to convert audio to text:</p>
      
      <pre>{`
from faster_whisper import WhisperModel

# Load model (tiny, base, small, medium, large)
model = WhisperModel("small", device="cpu", compute_type="int8")

def transcribe(audio_data):
    segments, info = model.transcribe(audio_data, beam_size=5)
    
    full_text = ""
    for segment in segments:
        full_text += segment.text + " "
        
    return full_text.strip()
      `}</pre>
      
      <h2>ğŸ¤– ROS 2 Node</h2>
      
      <p>Let's wrap this into a ROS 2 node that publishes the text:</p>
      
      <pre>{`
import rclpy
from rclpy.node import Node
from std_msgs.msg import String

class SpeechToTextNode(Node):
    def __init__(self):
        super().__init__('speech_to_text')
        self.publisher = self.create_publisher(String, '/voice/text', 10)
        self.timer = self.create_timer(1.0, self.listen_and_publish)
        self.model = WhisperModel("small", device="cpu", compute_type="int8")

    def listen_and_publish(self):
        # Record and transcribe (simplified)
        audio = record_audio()
        text = transcribe(audio)
        
        if text:
            msg = String()
            msg.data = text
            self.publisher.publish(msg)
            self.get_logger().info(f'Published: "{text}"')

def main():
    rclpy.init()
    node = SpeechToTextNode()
    rclpy.spin(node)
    node.destroy_node()
    rclpy.shutdown()
      `}</pre>
      
      <h2>ğŸŒ Multilingual Support</h2>
      
      <p>Whisper can automatically detect language. For Urdu, you can force it:</p>
      
      <pre>{`
segments, info = model.transcribe(audio, language="ur")
      `}</pre>
      
      <h2>ğŸ“š Next Steps</h2>
      
      <p>Now that the robot can hear, let's see how it can understand using <strong>LLMs</strong> â†’</p>
    </div>
  );
}
