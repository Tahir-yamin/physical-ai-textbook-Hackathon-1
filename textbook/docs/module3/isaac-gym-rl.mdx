---
sidebar_position: 3
---

import {useTranslation} from 'react-i18next';

export default function IsaacGymRL() {
  const {i18n} = useTranslation();
  const isUrdu = i18n.language === 'ur';
  
  if (isUrdu) {
    return (
      <div>
        <h1>Isaac Gym Ú©Û’ Ø³Ø§ØªÚ¾ Ú©Ù…Ú© Ø³ÛŒÚ©Ú¾Ù†Ø§ (Reinforcement Learning)</h1>
        
        <h2>ğŸ¯ Isaac Gym Ú©ÛŒØ§ ÛÛ’ØŸ</h2>
        
        <p><strong>Isaac Gym</strong> NVIDIA Ú©Ø§ Ø§ÛŒÚ© ÙØ±ÛŒÙ… ÙˆØ±Ú© ÛÛ’ Ø¬Ùˆ GPU Ù¾Ø± Ø¨Ú‘Û’ Ù¾ÛŒÙ…Ø§Ù†Û’ Ù¾Ø± Ù…ØªÙˆØ§Ø²ÛŒ Ú©Ù…Ú© Ø³ÛŒÚ©Ú¾Ù†Û’ (RL) Ú©ÛŒ Ø§Ø¬Ø§Ø²Øª Ø¯ÛŒØªØ§ ÛÛ’Û” Ø±ÙˆØ§ÛŒØªÛŒ Ø³Ù…ÛŒÙ„ÛŒÙ¹Ø±ÙˆÚº Ú©Û’ Ø¨Ø±Ø¹Ú©Ø³ Ø¬Ùˆ CPU Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÛŒÚºØŒ Isaac Gym ÛØ²Ø§Ø±ÙˆÚº Ø±ÙˆØ¨ÙˆÙ¹Ø³ Ú©Ùˆ Ø§ÛŒÚ© ÛÛŒ GPU Ù¾Ø± Ø¨ÛŒÚ© ÙˆÙ‚Øª Ù†Ù‚Ù„ Ú©Ø± Ø³Ú©ØªØ§ ÛÛ’ØŒ Ø¬Ø³ Ø³Û’ ØªØ±Ø¨ÛŒØª Ú©Ø§ ÙˆÙ‚Øª Ø¯Ù†ÙˆÚº Ø³Û’ Ù…Ù†Ù¹ÙˆÚº ØªÚ© Ú©Ù… ÛÙˆ Ø¬Ø§ØªØ§ ÛÛ’Û”</p>
        
        <h2>ğŸ—ï¸ Ø§ÛÙ… ØªØµÙˆØ±Ø§Øª</h2>
        
        <ul>
          <li><strong>Ù…Ø§Ø­ÙˆÙ„ (Environment)</strong>: ÙˆÛ Ø¯Ù†ÛŒØ§ Ø¬ÛØ§Úº Ø±ÙˆØ¨ÙˆÙ¹ Ø³ÛŒÚ©Ú¾ØªØ§ ÛÛ’ (Ø¬ÛŒØ³Û’ Ú©Ø§Ø±Ù¹ Ù¾ÙˆÙ„ØŒ ÛÛŒÙˆÙ…Ù†Ø§Ø¦Úˆ)Û”</li>
          <li><strong>Ø§ÛŒØ¬Ù†Ù¹ (Agent)</strong>: Ø±ÙˆØ¨ÙˆÙ¹ Ø¬Ùˆ Ø§Ø¹Ù…Ø§Ù„ Ø§Ù†Ø¬Ø§Ù… Ø¯ÛŒØªØ§ ÛÛ’Û”</li>
          <li><strong>Ø§Ù†Ø¹Ø§Ù… (Reward)</strong>: Ø§ÛŒØ¬Ù†Ù¹ Ú©Ùˆ Ø§Ú†Ú¾ÛŒ Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ Ù¾Ø± Ù…Ù„Ù†Û’ ÙˆØ§Ù„Ø§ Ø§Ø³Ú©ÙˆØ±Û”</li>
          <li><strong>Ù…Ø´Ø§ÛØ¯Û (Observation)</strong>: Ø§ÛŒØ¬Ù†Ù¹ Ú©Ùˆ Ù…Ø§Ø­ÙˆÙ„ Ú©Û’ Ø¨Ø§Ø±Û’ Ù…ÛŒÚº Ú©ÛŒØ§ Ù…Ø¹Ù„ÙˆÙ… ÛÛ’ (Ù¾ÙˆØ²ÛŒØ´Ù†ØŒ Ø±ÙØªØ§Ø±)Û”</li>
        </ul>
        
        <h2>ğŸ’» ØªÙ†ØµÛŒØ¨</h2>
        
        <p>Isaac Gym Ù¾ÛŒÚ©ÛŒØ¬ ÚˆØ§Ø¤Ù† Ù„ÙˆÚˆ Ú©Ø±ÛŒÚº Ø§ÙˆØ± Ø§Ù†Ø³Ù¹Ø§Ù„ Ú©Ø±ÛŒÚº:</p>
        
        <pre>{`
# Create conda environment
conda create -n rlgpu python=3.8
conda activate rlgpu

# Install PyTorch (ensure CUDA compatibility)
pip install torch torchvision

# Install Isaac Gym
cd isaacgym/python
pip install -e .
        `}</pre>
        
        <h2>ğŸš€ ØªØ±Ø¨ÛŒØª Ú©ÛŒ Ù…Ø«Ø§Ù„: Cartpole</h2>
        
        <p>Ø¢Ø¦ÛŒÛ’ Ø§ÛŒÚ© Ú©Ù„Ø§Ø³Ú© RL Ù…Ø³Ø¦Ù„Û Ú†Ù„Ø§ØªÛ’ ÛÛŒÚº: Ø§ÛŒÚ© Ú©Ú¾Ù…Ø¨Û’ Ú©Ùˆ Ú©Ø§Ø±Ù¹ Ù¾Ø± Ù…ØªÙˆØ§Ø²Ù† Ú©Ø±Ù†Ø§Û”</p>
        
        <pre>{`
cd isaacgym/python/examples
python train.py task=Cartpole
        `}</pre>
        
        <p>Ø¢Ù¾ Ø¯ÛŒÚ©Ú¾ÛŒÚº Ú¯Û’ Ú©Û ÛØ²Ø§Ø±ÙˆÚº Ú©Ø§Ø±Ù¹ Ù¾ÙˆÙ„Ø² Ù…ØªÙˆØ§Ø²ÛŒ Ø·ÙˆØ± Ù¾Ø± ØªØ±Ø¨ÛŒØª Ø­Ø§ØµÙ„ Ú©Ø± Ø±ÛÛ’ ÛÛŒÚº!</p>
        
        <h2>ğŸ§  Ø§Ù¾Ù†Ø§ RL Ù¹Ø§Ø³Ú© Ø¨Ù†Ø§Ù†Ø§</h2>
        
        <p>Ø§ÛŒÚ© Ø­Ø³Ø¨ Ø¶Ø±ÙˆØ±Øª Ù¹Ø§Ø³Ú© Ø¨Ù†Ø§Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ØŒ Ø¢Ù¾ Ú©Ùˆ <code>VecTask</code> Ú©Ù„Ø§Ø³ Ú©Ø§ ÙˆØ§Ø±Ø« ÛÙˆÙ†Ø§ Ú†Ø§ÛÛŒÛ’:</p>
        
        <pre>{`
from isaacgym import gymapi
from isaacgymenvs.tasks.base.vec_task import VecTask

class MyRobotTask(VecTask):
    def __init__(self, cfg, sim_device, graphics_device_id, headless):
        super().__init__(config=cfg, sim_device=sim_device, graphics_device_id=graphics_device_id, headless=headless)
        
        # Create simulation
        self.gym = gymapi.acquire_gym()
        self.sim = self.gym.create_sim(...)
        
    def compute_reward(self):
        # Define reward function
        # e.g., reward = 1.0 - distance_to_target
        pass
        
    def reset_idx(self, env_ids):
        # Reset robots that failed
        pass
        `}</pre>
        
        <h2>ğŸ“Š Sim-to-Real Ù¹Ø±Ø§Ù†Ø³ÙØ±</h2>
        
        <p>Ù†Ù‚Ù„ÛŒ Ù…ÛŒÚº ØªØ±Ø¨ÛŒØª ÛŒØ§ÙØªÛ Ù¾Ø§Ù„ÛŒØ³ÛŒ Ú©Ùˆ Ø­Ù‚ÛŒÙ‚ÛŒ Ø±ÙˆØ¨ÙˆÙ¹ Ù…ÛŒÚº Ù…Ù†ØªÙ‚Ù„ Ú©Ø±Ù†Ø§ Ù…Ø´Ú©Ù„ ÛÛ’Û” Isaac Gym Ø§Ø³ Ù…ÛŒÚº Ù…Ø¯Ø¯ Ú©Ø±ØªØ§ ÛÛ’:</p>
        
        <ul>
          <li><strong>ÚˆÙˆÙ…ÛŒÙ† Ø±ÛŒÙ†ÚˆÙ…Ø§Ø¦Ø²ÛŒØ´Ù†</strong>: Ø±Ú¯Ú‘ØŒ Ø¨Ú‘Û’ Ù¾ÛŒÙ…Ø§Ù†Û’ Ù¾Ø±ØŒ Ø§ÙˆØ± Ø¨ØµØ±ÛŒ Ø®ØµÙˆØµÛŒØ§Øª Ú©Ùˆ ØªØµØ§Ø¯ÙÛŒ Ø·ÙˆØ± Ù¾Ø± ØªØ¨Ø¯ÛŒÙ„ Ú©Ø±Ù†Ø§ ØªØ§Ú©Û Ù¾Ø§Ù„ÛŒØ³ÛŒ Ù…Ø¶Ø¨ÙˆØ· ÛÙˆÛ”</li>
          <li><strong>Ø´ÙˆØ± (Noise)</strong>: Ø³ÛŒÙ†Ø³Ø± ÚˆÛŒÙ¹Ø§ Ù…ÛŒÚº Ø´ÙˆØ± Ø´Ø§Ù…Ù„ Ú©Ø±Ù†Ø§Û”</li>
        </ul>
        
        <h2>ğŸ› ï¸ Ø¨ÛØªØ±ÛŒÙ† Ø·Ø±ÛŒÙ‚Û’</h2>
        
        <ol>
          <li><strong>Ø§Ù†Ø¹Ø§Ù… Ú©ÛŒ ØªØ´Ú©ÛŒÙ„ (Reward Shaping)</strong>: Ø§ÛŒÚ© Ø§Ú†Ú¾Ø§ Ø§Ù†Ø¹Ø§Ù… ÙÙ†Ú©Ø´Ù† Ø¨ÛØª Ø¶Ø±ÙˆØ±ÛŒ ÛÛ’Û” Ú†Ú¾ÙˆÙ¹Û’ØŒ Ù…Ø³Ù„Ø³Ù„ Ø§Ù†Ø¹Ø§Ù…Ø§Øª (Ø¬ÛŒØ³Û’ ÛØ¯Ù Ú©ÛŒ Ø·Ø±Ù Ù¾ÛŒØ´Ø±ÙØª) Ø¨Ú‘Û’ØŒ ÙˆÛŒØ±Ù„ Ø§Ù†Ø¹Ø§Ù…Ø§Øª (Ø¬ÛŒØ³Û’ ØµØ±Ù Ø¬ÛŒØªÙ†Û’ Ù¾Ø±) Ø³Û’ Ø¨ÛØªØ± ÛÛŒÚºÛ”</li>
          <li><strong>Ù†ØµØ§Ø¨ Ú©ÛŒ ØªØ¹Ù„ÛŒÙ… (Curriculum Learning)</strong>: Ø¢Ø³Ø§Ù† Ú©Ø§Ù…ÙˆÚº Ø³Û’ Ø´Ø±ÙˆØ¹ Ú©Ø±ÛŒÚº Ø§ÙˆØ± Ø¢ÛØ³ØªÛ Ø¢ÛØ³ØªÛ Ù…Ø´Ú©Ù„ Ù…ÛŒÚº Ø§Ø¶Ø§ÙÛ Ú©Ø±ÛŒÚºÛ”</li>
        </ol>
        
        <h2>ğŸ“š Ø§Ú¯Ù„Û’ Ø§Ù‚Ø¯Ø§Ù…Ø§Øª</h2>
        
        <p>Ø§Ø¨ Ø¬Ø¨ Ú©Û Ø¢Ù¾ Ú©Û’ Ù¾Ø§Ø³ Ø§ÛŒÚ© ÛÙˆØ´ÛŒØ§Ø± Ø§ÛŒØ¬Ù†Ù¹ ÛÛ’ØŒ Ø¢Ø¦ÛŒÛ’ Ø¯ÛŒÚ©Ú¾ØªÛ’ ÛÛŒÚº Ú©Û <strong>Omniverse Replicator</strong> Ú©Û’ Ø³Ø§ØªÚ¾ Ø§Ø³ Ú©Û’ Ù„ÛŒÛ’ ØªØ±Ø¨ÛŒØªÛŒ ÚˆÛŒÙ¹Ø§ Ú©ÛŒØ³Û’ ØªÛŒØ§Ø± Ú©ÛŒØ§ Ø¬Ø§Ø¦Û’ â†’</p>
      </div>
    );
  }
  
  return (
    <div>
      <h1>Reinforcement Learning with Isaac Gym</h1>
      
      <h2>ğŸ¯ What is Isaac Gym?</h2>
      
      <p><strong>Isaac Gym</strong> is NVIDIA's framework for massive parallel reinforcement learning (RL) on GPU. Unlike traditional simulators that use CPU, Isaac Gym can simulate thousands of robots simultaneously on a single GPU, reducing training time from days to minutes.</p>
      
      <h2>ğŸ—ï¸ Key Concepts</h2>
      
      <ul>
        <li><strong>Environment</strong>: The world where the robot learns (e.g., Cartpole, Humanoid).</li>
        <li><strong>Agent</strong>: The robot performing actions.</li>
        <li><strong>Reward</strong>: The score the agent gets for performing well.</li>
        <li><strong>Observation</strong>: What the agent knows about the environment (position, velocity).</li>
      </ul>
      
      <h2>ğŸ’» Installation</h2>
      
      <p>Download and install the Isaac Gym package:</p>
      
      <pre>{`
# Create conda environment
conda create -n rlgpu python=3.8
conda activate rlgpu

# Install PyTorch (ensure CUDA compatibility)
pip install torch torchvision

# Install Isaac Gym
cd isaacgym/python
pip install -e .
      `}</pre>
      
      <h2>ğŸš€ Training Example: Cartpole</h2>
      
      <p>Let's run a classic RL problem: balancing a pole on a cart.</p>
      
      <pre>{`
cd isaacgym/python/examples
python train.py task=Cartpole
      `}</pre>
      
      <p>You will see thousands of cartpoles training in parallel!</p>
      
      <h2>ğŸ§  Creating Your Own RL Task</h2>
      
      <p>To create a custom task, you inherit from the <code>VecTask</code> class:</p>
      
      <pre>{`
from isaacgym import gymapi
from isaacgymenvs.tasks.base.vec_task import VecTask

class MyRobotTask(VecTask):
    def __init__(self, cfg, sim_device, graphics_device_id, headless):
        super().__init__(config=cfg, sim_device=sim_device, graphics_device_id=graphics_device_id, headless=headless)
        
        # Create simulation
        self.gym = gymapi.acquire_gym()
        self.sim = self.gym.create_sim(...)
        
    def compute_reward(self):
        # Define reward function
        # e.g., reward = 1.0 - distance_to_target
        pass
        
    def reset_idx(self, env_ids):
        # Reset robots that failed
        pass
      `}</pre>
      
      <h2>ğŸ“Š Sim-to-Real Transfer</h2>
      
      <p>Transferring a policy trained in simulation to a real robot is challenging. Isaac Gym helps with:</p>
      
      <ul>
        <li><strong>Domain Randomization</strong>: Randomizing friction, mass, and visual properties to make the policy robust.</li>
        <li><strong>Noise</strong>: Adding noise to sensor data.</li>
      </ul>
      
      <h2>ğŸ› ï¸ Best Practices</h2>
      
      <ol>
        <li><strong>Reward Shaping</strong>: A good reward function is crucial. Small, continuous rewards (like progress towards goal) are better than large, sparse rewards (like only on win).</li>
        <li><strong>Curriculum Learning</strong>: Start with easy tasks and gradually increase difficulty.</li>
      </ol>
      
      <h2>ğŸ“š Next Steps</h2>
      
      <p>Now that you have a smart agent, let's see how to generate training data for it with <strong>Omniverse Replicator</strong> â†’</p>
    </div>
  );
}
